{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import urllib\n",
    "#import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/raw/train.csv'\n",
    "train_path = '/raw/train.csv'\n",
    "\n",
    "cur_wd = os.getcwd()\n",
    "\n",
    "file_path = cur_wd + relative_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set_data(target_month, outlier_cutoff, income_cutoff, path):\n",
    "    relative_path = path\n",
    "    cur_wd = os.getcwd()\n",
    "    file_path = cur_wd + relative_path\n",
    "\n",
    "    if not Path(file_path).exists():\n",
    "        print('Filepath does not exist in the \"raw\" folder. Check again')\n",
    "        \n",
    "    else:\n",
    "        print('Loading and cleaning data.')\n",
    "\n",
    "    d_types = {'ID': str, 'Customer_ID': str, 'Month': str, 'Name': str, 'Age': int, 'SSN': str, 'Occupation': str, 'Annual_Income': str, 'Monthly_Inhand_Salary': float, 'Num_Bank_Accounts': int, 'Num_Credit_Card': int, 'Interest_Rate': int, 'Num_of_Loan': int,'Type_of_Loan': str,\n",
    "'Delay_from_due_date': int, 'Num_of_Delayed_Payment': int, 'Changed_Credit_Limit': float, 'Num_Credit_Inquiries': float, 'Credit_Mix': str, 'Outstanding_Debt': str, 'Credit_Utilization_Ratio': float, 'Credit_History_Age': float, 'Payment_of_Min_Amount': str, 'Total_EMI_per_month': float,\n",
    "       'Amount_invested_monthly': float, 'Payment_Behaviour': str, 'Monthly_Balance':float, 'Credit_Score':str}\n",
    "\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Data cleaning, type conversion and filtering DataFrame\n",
    "    # remove underscores and convert 'Annual Income' column from string to float\n",
    "    df['Annual_Income'] = df['Annual_Income'].str.replace('_', '').astype(float) \n",
    "\n",
    "    # filter out rows where 'Num_Credit_Inquiries' is blank and convert to integer\n",
    "    df = df[df['Num_Credit_Inquiries'].notna()]\n",
    "    df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].astype(int)\n",
    "    \n",
    "    # filter out rows where 'Credit_Mix' has underscores\n",
    "    df = df[~df['Credit_Mix'].str.contains('_')]\n",
    "\n",
    "    # remove underscores and convert 'Outstanding_Debt' column from string to float\n",
    "    df['Outstanding_Debt'] = df['Outstanding_Debt'].str.replace('_', '').astype(float)\n",
    "\n",
    "    # no changes required for \"Credit_Utilization_Ratio\"\n",
    "\n",
    "    # further filtering based on specific conditions\n",
    "    df['Amount_invested_monthly'] = df['Amount_invested_monthly'].str.replace('_', '').astype(float)\n",
    "    single_month_frame = df[df['Month'] ==  target_month]\n",
    "    single_month_frame = single_month_frame[single_month_frame['Type_of_Loan'] !=  'Auto Loan']\n",
    "    single_month_frame = single_month_frame[single_month_frame['Amount_invested_monthly'] < outlier_cutoff]\n",
    "    single_month_frame = single_month_frame[single_month_frame['Annual_Income'] < income_cutoff]\n",
    "    \n",
    "\n",
    "    return single_month_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiya\\AppData\\Local\\Temp\\ipykernel_31312\\4217207163.py:17: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for April: 8782\n"
     ]
    }
   ],
   "source": [
    "# checking data cleaning, type conversion and filtering DataFrame for April\n",
    "\n",
    "# Load data for April with specified cutoffs\n",
    "target_month = 'April'\n",
    "outlier_cutoff = 10000\n",
    "income_cutoff = 1000000\n",
    "path = train_path\n",
    "\n",
    "# Call the function to get the filtered DataFrame\n",
    "april_data = load_set_data(target_month, outlier_cutoff, income_cutoff, path)\n",
    "\n",
    "# Print the number of rows\n",
    "print(f'Number of rows for {target_month}:', len(april_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF CLEAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiya\\AppData\\Local\\Temp\\ipykernel_31312\\391738695.py:3: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd_cs_class = pd.read_csv(file_path_cs_class)\n"
     ]
    }
   ],
   "source": [
    "file_path_cs_class =  'raw/train.csv' \n",
    "\n",
    "pd_cs_class = pd.read_csv(file_path_cs_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Data cleaning, type conversion and filtering DataFrame\n",
    "# remove underscores and convert 'Annual Income' column from string to float\n",
    "pd_cs_class['Annual_Income'] = pd_cs_class['Annual_Income'].str.replace('_', '').astype(float) \n",
    "\n",
    "# filter out rows where 'Num_Credit_Inquiries' is blank and convert to integer\n",
    "pd_cs_class = pd_cs_class[pd_cs_class['Num_Credit_Inquiries'].notna()]\n",
    "pd_cs_class['Num_Credit_Inquiries'] = pd_cs_class['Num_Credit_Inquiries'].astype(int)\n",
    "\n",
    "# filter out rows where 'Credit_Mix' has underscores\n",
    "pd_cs_class = pd_cs_class[~pd_cs_class['Credit_Mix'].str.contains('_')]\n",
    "\n",
    "# remove underscores and convert 'Outstanding_Debt' column from string to float\n",
    "pd_cs_class['Outstanding_Debt'] = pd_cs_class['Outstanding_Debt'].str.replace('_', '').astype(float)\n",
    "\n",
    "\n",
    "pd_cs_class = pd_cs_class[pd_cs_class['Annual_Income'] < income_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_cs_class.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Annual_Income','Num_Credit_Inquiries','Outstanding_Debt','Annual_Income',\n",
    "            'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', \n",
    "            'Delay_from_due_date', 'Num_Credit_Inquiries', 'Credit_Utilization_Ratio', 'Total_EMI_per_month' ]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pd_cs_class[features] = scaler.fit_transform(pd_cs_class[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd_cs_class[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd_cs_class['Credit_Score']\n",
    "cs_mapping = {\n",
    "    'Good': 3,\n",
    "    'Standard':2,\n",
    "    'Poor':1\n",
    "}\n",
    "df_y = df_y.map(cs_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_X,df_y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Keras version: 2.10.0\n",
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Set random seed for reproducability\n",
    "tensorflow.random.set_seed(0)\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "print('Keras version:',keras.__version__)\n",
    "print('TensorFlow version:',tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready...\n"
     ]
    }
   ],
   "source": [
    "# Set data types for float features\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Set data types for categorical labels\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "print('Ready...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                130       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 284\n",
      "Trainable params: 284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a classifier network\n",
    "hl = 10 # Number of hidden layer nodes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hl, input_dim=len(features), activation='relu'))\n",
    "model.add(Dense(hl, input_dim=hl, activation='relu'))\n",
    "model.add(Dense(4, input_dim=hl, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameters for optimizer\n",
    "learning_rate = 0.001\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
    "num_epochs = 50\n",
    "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "epoch_nums = range(1,num_epochs+1)\n",
    "training_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()[0]\n",
    "    biases = layer.get_weights()[1]\n",
    "    print('------------\\nWeights:\\n',weights,'\\nBiases:\\n', biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class_probabilities = model.predict(x_test)\n",
    "predictions = np.argmax(class_probabilities, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(3)\n",
    "plt.xticks(tick_marks,  rotation=85)\n",
    "plt.yticks(tick_marks, )\n",
    "plt.xlabel(\"Predicted Score\")\n",
    "plt.ylabel(\"Actual Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
